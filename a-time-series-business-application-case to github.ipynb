{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of this notebook\n",
    "\n",
    "The objective of this notebook is to try to show how do I think when I develop a ML model, this attempt will be done through a simple and complete approach to time-series problem with linear regression and Fourier series. For this, I will draw some dimensions this notebook will/won't cover:\n",
    "\n",
    "- I will do an applied (and fictional) business case of this project. However, for this I will need to do some assumptions explained later.\n",
    "- We will compare typical time-series base line models (last observation, historical mean, and historical median) vs multivariate linear regression (with deterministic processes)\n",
    "- This notebook won't cover tree-based models as the final increase in performance obtained are sufficient for a business case application. However, in a future I will do a purely-technical approach comparing the models in this projects with tree-based models + trend (as tree based models don't overpolate), and LSTM.\n",
    "\n",
    "That said, let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business application\n",
    "\n",
    "### Before start, I will do two strong assumptions:\n",
    "\n",
    "1. As in business, usually we (as Data Scientists) aren't asked to predict something specific but, on the contrary, we face multiple business problems and business opportunities, where some of them could be solvable with machine learning (ML), where some of those that are solvable with ML could have some feasible solution, and maybe one of those feasible solutions could be worth of working on (a solution that's worth is just a solution which potential income is significantly higher than potential costs). So before getting this data, we are assuming that we already did that whole process.\n",
    "2. The second assumption, is that once we have our business problem well defined, which means that we know that's solvable with ML, (potentially) feasible and (potentially) profitable.. we got the data ready!. We assume that we already have had plenty of meetings with the data team, BI team, business partner teams, in order to answer the question of the first assumption and to understand:\n",
    "- Data: What's the data structure of the company? How is the data consistency throughtout time? What features can we use? Are those features available at the time of production?\n",
    "- Using the ML model: How the ML model will be consumed? How much time pass between the business side receives our predictions and take some business action? How are we going to measure its real-time performance? Do we need continuous training or we will  do one-time training and we will update the ML model once the metrics performances are down from a pre-defined threshold?\n",
    "- Business features: What features the business want to include? Those features fulfill the minimal requirements of consintency and avaliability to include them? \n",
    "- Missing data: Do we have missing data when building the datasets (train, test, hold-out, and implementation dataset)? If we have missing data, do we have any business insight to impute those missing values? We always can impute missing data with technical related techniques but usually we can find business-related reasons for the missing data and imputing them directly without any headache.\n",
    "\n",
    "Given those two strong assumptions, we got a clean dataset and a confident level that the model we are going to develop and deploy is going to be used (if it adds value, of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's start!\n",
    "\n",
    "I picked up an airline monthly passengers dataset with the monthly quantity of passengers as the only variable from 1949 until 1960. But before we get into analyzing data and modeling, we have to ask first:\n",
    "\n",
    "**What is the most appropriate metric model evaluation for this business problem?**\n",
    "\n",
    "As we need to predict future demand, also we need to choose a metric to measure our technical performance. There is multiple metrics to measure regression models but the question we need to answer is: what's the evaluation metric that makes more sense for this business problem? \n",
    "\n",
    "Airlines are allowed to overbook and bumping, in this [US Department of Transportation site](https://www.transportation.gov/individuals/aviation-consumer-protection/bumping-oversales) we can read plenty of advantages for airlines to overbooking and bump passengers. If we get into the airlines perspective given this context, we are facing the decision trade-off of not overbooking and risking a flight with empty seats, or overbooking and risking giving compensations to passengers bumped. Given that airlines tend to overbook, we can assume that airlines prefer to risk to compensate for a passenger bumped, which means that for airlines is more costly to fly with empty seats than an overbooked flight risking paid compensations. With this, we can say that is more costly for an airline to underestimate demand than to overestimate demand. The mean squared logarithmic error (MSLE) is a regression evaluation metric that penalizes underestimates more than overestimates, which goes in line with the airlines decision trade-off we just discussed. Hence, we will use **MSLE as our main decision metric**, and we will take **mean absolute error (MAE)** and **mean squared error (MSE)** as a reference metrics evaluations.\n",
    "\n",
    "Having decided on that, let's start with the setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:39.601470Z",
     "iopub.status.busy": "2022-09-07T19:28:39.600708Z",
     "iopub.status.idle": "2022-09-07T19:28:39.609295Z",
     "shell.execute_reply": "2022-09-07T19:28:39.608421Z",
     "shell.execute_reply.started": "2022-09-07T19:28:39.601414Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.signal import periodogram\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_log_error as MSLE, mean_squared_error as MSE, mean_absolute_error as MAE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions created for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:39.611868Z",
     "iopub.status.busy": "2022-09-07T19:28:39.611209Z",
     "iopub.status.idle": "2022-09-07T19:28:39.629018Z",
     "shell.execute_reply": "2022-09-07T19:28:39.627946Z",
     "shell.execute_reply.started": "2022-09-07T19:28:39.611836Z"
    }
   },
   "outputs": [],
   "source": [
    "def ts_train_test_split(X_data, y_data, cols):\n",
    "    '''\n",
    "    This functions does a time series split where the last two years \n",
    "    will be the test set, and all the years before will be the training set\n",
    "    '''\n",
    "    latest_two_years = X_data.index.year.unique().sort_values()[-2:]\n",
    "    \n",
    "    train_mask = ~X_data.index.year.isin(latest_two_years)\n",
    "    test_mask = X_data.index.year.isin(latest_two_years)\n",
    "    \n",
    "    X_train = X_data.loc[train_mask, cols]\n",
    "    y_train = y_data[train_mask]\n",
    "    X_test = X_data.loc[test_mask, cols]\n",
    "    y_test = y_data[test_mask]\n",
    "    \n",
    "    return(X_train, y_train, X_test, y_test)\n",
    "\n",
    "def metrics_computation(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    '''\n",
    "    This function compute each of the three different\n",
    "    metrics (MSLE, MSE, MAE) we defined, for each model we try.\n",
    "    '''\n",
    "    error_train = (y_train-y_pred_train).rename('Train Error')\n",
    "    error_test = (y_test-y_pred_test).rename('Test Error')\n",
    "    \n",
    "    mae_train = MAE(y_train, y_pred_train).round(5)\n",
    "    mae_test = MAE(y_test, y_pred_test).round(5)\n",
    "    \n",
    "    mse_train = MSE(y_train, y_pred_train).round(5)\n",
    "    mse_test = MSE(y_test, y_pred_test).round(5)\n",
    "    \n",
    "    msle_train = MSLE(y_train, y_pred_train).round(5)\n",
    "    msle_test = MSLE(y_test, y_pred_test).round(5)\n",
    "    \n",
    "    error_train_div = (y_train/y_pred_train).rename('Train Error')\n",
    "    \n",
    "    df_errors = pd.DataFrame({'metrics': ['MSLE train', 'MSLE test','MSE train', 'MSE test', 'MAE train', 'MAE test'],\n",
    "                  'values': [msle_train, msle_test, mse_train, mse_test, mae_train, mae_test]})\n",
    "    \n",
    "    return error_train, error_test, mae_train, mae_test, mse_train, mse_test, msle_train, msle_test, df_errors\n",
    "\n",
    "def plot_metrics_model(cols, y_data, y_pred_train, y_pred_test, error_train, error_test,\n",
    "                       msle_train, msle_test, mse_train, mse_test, mae_train, mae_test, df_errors, title=None):\n",
    "    \"\"\"\n",
    "    This  function plots the train and test error \n",
    "    we computed in the metrics_computation function.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(18,4))\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax2 = fig.add_subplot(223)\n",
    "    ax3 = fig.add_subplot(122)\n",
    "    \n",
    "    ax3.table(cellText=df_errors.values, colLabels=df_errors.columns, loc='center')\n",
    "    \n",
    "    fig.suptitle(f'{title}; Columns used: {cols}') \n",
    "    ax1 = y_data.plot(label='Original data', ax=ax1, c='blue')\n",
    "    ax1 = y_pred_train.plot(ax=ax1, c='orange')\n",
    "    ax1 = y_pred_test.plot(ax=ax1, c='orange', linestyle='--')\n",
    "    ax1.legend()\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_xlabel('')\n",
    "\n",
    "    ax2 = error_train.plot(label='Train error', ax=ax2)\n",
    "    ax2 = error_test.plot(label='Test error', ax=ax2, linestyle='--')\n",
    "    ax2.legend()\n",
    "    ax3.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:39.631563Z",
     "iopub.status.busy": "2022-09-07T19:28:39.630776Z",
     "iopub.status.idle": "2022-09-07T19:28:39.843320Z",
     "shell.execute_reply": "2022-09-07T19:28:39.842223Z",
     "shell.execute_reply.started": "2022-09-07T19:28:39.631520Z"
    }
   },
   "outputs": [],
   "source": [
    "df_airps = pd.read_csv('../input/air-passengers/AirPassengers.csv')\n",
    "df_airps.columns = ['Month', 'Pax']\n",
    "df_airps.index = pd.to_datetime(df_airps.Month).dt.to_period('M')\n",
    "df_airps = df_airps.drop('Month', axis=1)\n",
    "df_airps.Pax.plot(figsize=(10,3))\n",
    "plt.title('Original data')\n",
    "plt.show()\n",
    "y = df_airps.squeeze().rename('Y')\n",
    "display(df_airps.head(3))\n",
    "print('Total missing values in the dataset:', df_airps.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consist of monthly air passengers. We can see that the time series data is multiplicative, has annual seasonality, a positive trend (more linear than squared seems to be), and no missing values.\n",
    "\n",
    "We will assess our dependent variable with:\n",
    "- **Periodogram**: It will tell us the stationality frequency (which from the plot above we can see it's likely will be annual).\n",
    "- **Partial autocorrelation**: It measures the residual correlation between lags features and the dependent variable (Y) once the influence of the most recent lags is removed. For example, given that lag_3 represent the dependent variable (Y) lagged 3 periods, and their partial correlation is the correlation between lag_3 and Y removing the influence of the correlation between Y_1 and Y, and the partial correlation between Y_2 and Y, where the partial correlation between Y_2 and Y is the correlation between Y and Y_2 minus the influence of the correlation between Y and Y_1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:39.845188Z",
     "iopub.status.busy": "2022-09-07T19:28:39.844752Z",
     "iopub.status.idle": "2022-09-07T19:28:40.144441Z",
     "shell.execute_reply": "2022-09-07T19:28:40.143273Z",
     "shell.execute_reply.started": "2022-09-07T19:28:39.845154Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_periodogram(ts, fs):\n",
    "    '''\n",
    "    This function plots the signal strenght distribution \n",
    "    of the time series through time\n",
    "    '''\n",
    "    frequencies, spectrum = periodogram(\n",
    "        ts,\n",
    "        fs=fs,\n",
    "        detrend='linear',\n",
    "        window=\"boxcar\",\n",
    "        scaling='spectrum',\n",
    "    )\n",
    "    _, ax = plt.subplots()\n",
    "    ax.step(frequencies, spectrum, color=\"blue\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([1, 2, 4, 6])\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"Annual (1)\",\n",
    "            \"Semiannual (2)\",\n",
    "            \"Quarterly (4)\",\n",
    "            \"Bimonthly (6)\",\n",
    "            #\"Monthly (12)\",\n",
    "            #\"Biweekly (26)\",\n",
    "            #\"Weekly (52)\",\n",
    "            #\"Semiweekly (104)\",\n",
    "        ],\n",
    "        rotation=90,\n",
    "    )\n",
    "    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "    ax.set_title(\"Periodogram\")\n",
    "    plt.show()\n",
    "\n",
    "plot_periodogram(y, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:40.148749Z",
     "iopub.status.busy": "2022-09-07T19:28:40.148101Z",
     "iopub.status.idle": "2022-09-07T19:28:40.353941Z",
     "shell.execute_reply": "2022-09-07T19:28:40.352889Z",
     "shell.execute_reply.started": "2022-09-07T19:28:40.148703Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,4))\n",
    "plot_pacf(y, method= 'ywm',lags = 24, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above, we can seee that we have a strong annual seasonality and a significant partial autocorrelation of our dependent variable with lags 1, 2, 9 and 13. If we werent sure that our series is stationary we could apply the [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test), which is a statistical hypothesis test that allows us to detect stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Given the analysis above, we will do create the next features:\n",
    "- `Y_Lag1`, `Y_Lag2`, `Y_Lag9`, `Y_Lag13`: The dependent variable lagged for the periods we found in the periodogram. Y_Lag1 is our last observation baseline model.\n",
    "- `Y_median_12m`: The rolling median for 12 periods before, this is ous historial median baseline model\n",
    "- `Y_mean_12m`: The rolling mean for 12 periods before, this is our historical mean baseline model\n",
    "- `const`, `trend`, and `trend_squared`: the constant is needed as we using linear regression and we have certainty that the prediction doesn't start at the origin. trend and trend_squared because I can't see if the time series is better fitted with a straight line or a mix of straight and trend squared.\n",
    "- Seasonal variables: For annual seasonality buy for each month\n",
    "- `Fourier` series: A fourier series of one frequency per year, as we saw in the plot we have an annual seasonality.\n",
    "- `sin_tr`, `cos_tr`: Fourier series multiplied by the trend, this will allow us to capture the multiplicative effect.\n",
    "- `sin_m12w`, `cos_m12w`: Idem, but multiplied by the historical mean (as the historical mean is better to capture when the trend is not straight).\n",
    "\n",
    "At work we could be creative and get some independent features. For example, could be nice to have the [US consumer confidence index](https://www.conference-board.org/topics/consumer-confidence) as it's a leading indicator about consumption intentions of the US citizen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:40.355647Z",
     "iopub.status.busy": "2022-09-07T19:28:40.355323Z",
     "iopub.status.idle": "2022-09-07T19:28:40.445833Z",
     "shell.execute_reply": "2022-09-07T19:28:40.444687Z",
     "shell.execute_reply.started": "2022-09-07T19:28:40.355616Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(y.shift(1).bfill().rename('Y_Lag1'))\n",
    "X['Y_Lag2'] = y.shift(2).bfill()\n",
    "X['Y_Lag9'] = y.shift(9).bfill()\n",
    "X['Y_Lag13'] = y.shift(13).bfill()\n",
    "X['Y_median_12m'] = y.rolling(window=12, min_periods=3).median().shift(1).bfill() # We need to shift the data as the laterolling\n",
    "X['Y_mean_12m'] = y.rolling(window=12, min_periods=3).mean().shift(1).bfill()\n",
    "\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "        index=y.index,\n",
    "        constant=True,\n",
    "        order=2, # Trend and trend squared\n",
    "        seasonal=True, # Seasonal variables\n",
    "        additional_terms=[CalendarFourier(freq=\"Y\", order=1)], # Fourier series\n",
    "        drop=True,\n",
    "    )\n",
    "\n",
    "X = pd.merge(X, dp.in_sample(), left_index=True, right_index=True, how='left')\n",
    "\n",
    "cols_fourier = ['sin(1,freq=A-DEC)', 'cos(1,freq=A-DEC)']\n",
    "\n",
    "X = X.rename(columns = {'sin(1,freq=A-DEC)':'sin', 'cos(1,freq=A-DEC)':'cos'})\n",
    "X['sin_tr'] = X.trend*X.sin\n",
    "X['cos_tr'] = X.trend*X.cos\n",
    "X['sin_m12w'] = X.Y_mean_12m*X.sin\n",
    "X['cos_m12w'] = X.Y_mean_12m*X.cos\n",
    "seasonal_cols = ['s(2,12)','s(3,12)','s(4,12)','s(5,12)','s(6,12)',\n",
    "                 's(7,12)','s(8,12)','s(9,12)','s(10,12)','s(11,12)','s(12,12)']\n",
    "pd.concat([X,y], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for demonstratation purposes, we will see how the frequency our Fourier series fits the time series detrended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:40.447657Z",
     "iopub.status.busy": "2022-09-07T19:28:40.447288Z",
     "iopub.status.idle": "2022-09-07T19:28:40.646225Z",
     "shell.execute_reply": "2022-09-07T19:28:40.644960Z",
     "shell.execute_reply.started": "2022-09-07T19:28:40.447623Z"
    }
   },
   "outputs": [],
   "source": [
    "data = DeterministicProcess(\n",
    "        index=y.index,\n",
    "        constant=True,\n",
    "        order=1, # Trend and trend squared\n",
    "        seasonal=False,\n",
    "        additional_terms=[CalendarFourier(freq=\"Y\", order=1)], # Fourier series, one frequency per year\n",
    "        drop=True,\n",
    "    ).in_sample()\n",
    "data\n",
    "\n",
    "model = LinearRegression(fit_intercept=False).fit(data[['const', 'trend']], y)\n",
    "y_pred = pd.Series(model.predict(data[['const', 'trend']]), index=data.index, name='Y_pred')\n",
    "y_detrended = y-y_pred\n",
    "\n",
    "model = LinearRegression(fit_intercept=False).fit(data[['const', 'sin(1,freq=A-DEC)','cos(1,freq=A-DEC)']], y_detrended)\n",
    "y_pred = pd.Series(model.predict(data[['const', 'sin(1,freq=A-DEC)','cos(1,freq=A-DEC)']]), index=data.index, name='Y_pred')\n",
    "\n",
    "y_detrended.plot(label='Y_detrended')\n",
    "y_pred.plot(label='Fourier prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the fourier series fit nicely the frequency seasonality but still requires an incremental factor to fit better the time series data, that's why we are multipliying the fourier series with the trend in the feature engineering step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series split discussion\n",
    "\n",
    "At the beginning of this notebook I created the function `ts_train_test_split()` which separates the train and test data, giving the test data the last two years and the rest of the years the training set.\n",
    "\n",
    "In real-life projects **we split the data in line the business requirements** as feasible it can be. Does the business need yearly, monthly, weekly, or daily predictions? (In this project we couldn't do higher than monthly prediction as our data period is in months, so we would need to ask data with the level of granularity that the business requires). After we define the frequency which we will make our predictions (and hence we will split the data given that criteria), we need to ask ourselves: **how much time needs the business from receiving our input to executing business actions?** We could have no delay in our data availability for production**[1]** but if the business needs one week to plan those actions, we need to consider that week when we are modeling as it will impact in our model in production. \n",
    "\n",
    "For example, if we are able to deliver monthly predictions each 30th on each month, and the business takes 10 days to take action after receiving our input, our predictions would be impacting the business from 10th on the month after we delivered our predictions, missing those 10 days we  predicted. So, to make a correct implementation in this example, we would have to deliver the prediction each 20th of each month, so the business uses the have those 10 needed days to take someaction on the 1st of the next month. \n",
    "\n",
    "**[1]** Having no delay in our data availibility for production means that all of our features are available the day we deliver the predictions to the business. For example, sometimes you can have a delay because the banking data (assuming you have a feature from banking data) every month is uploaded to the databases the 8th of the next month. Hence, you can't predict for 1st to 30th of September using the data of August because that data won't be available then we do the predicitons. You will have to predict with the data of July, having one month delay in your data availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models: historical mean, historical median, and last observation\n",
    "\n",
    "Here we build the three baseline models and their respective errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:40.648753Z",
     "iopub.status.busy": "2022-09-07T19:28:40.648005Z",
     "iopub.status.idle": "2022-09-07T19:28:40.658277Z",
     "shell.execute_reply": "2022-09-07T19:28:40.656850Z",
     "shell.execute_reply.started": "2022-09-07T19:28:40.648696Z"
    }
   },
   "outputs": [],
   "source": [
    "def mdl_baseline(X_data, y_data, cols, title, ret=False):\n",
    "    '''\n",
    "    It computes, calculates, and plot the baseline model.\n",
    "    '''\n",
    "    X_train, y_train, X_test, y_test = ts_train_test_split(X_data, y_data, cols)\n",
    "    \n",
    "    y_pred_train = X_train[cols].squeeze()\n",
    "    y_pred_test = X_test[cols].squeeze()\n",
    "    \n",
    "    error_train, error_test, mae_train, mae_test, mse_train, mse_test, msle_train, msle_test, df_errors = metrics_computation(y_train, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "    plot_metrics_model(cols, y_data, y_pred_train, y_pred_test, error_train, error_test,\n",
    "                       msle_train, msle_test, mse_train, mse_test, mae_train, mae_test, df_errors, title)\n",
    "    \n",
    "    if ret==True:\n",
    "        return(df_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:40.661406Z",
     "iopub.status.busy": "2022-09-07T19:28:40.660152Z",
     "iopub.status.idle": "2022-09-07T19:28:42.286134Z",
     "shell.execute_reply": "2022-09-07T19:28:42.285138Z",
     "shell.execute_reply.started": "2022-09-07T19:28:40.661359Z"
    }
   },
   "outputs": [],
   "source": [
    "errors_lo = mdl_baseline(X, y, ['Y_Lag1'], 'Last Observation baseline model', ret=True)\n",
    "errors_hmean = mdl_baseline(X, y, ['Y_median_12m'], 'Historical mean baseline model', ret=True)\n",
    "errors_hmedian = mdl_baseline(X, y, ['Y_mean_12m'], 'Historical median baseline model', ret=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that our best baseline model is the Last Observation, with MSLE 1.1% in train and 1.2% in test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:42.297532Z",
     "iopub.status.busy": "2022-09-07T19:28:42.294674Z",
     "iopub.status.idle": "2022-09-07T19:28:42.315177Z",
     "shell.execute_reply": "2022-09-07T19:28:42.313937Z",
     "shell.execute_reply.started": "2022-09-07T19:28:42.297479Z"
    }
   },
   "outputs": [],
   "source": [
    "def mdl_linear_reg(X_data, y_data, cols, title, ret=False):\n",
    "    '''\n",
    "    This function will allow us to efficiently do multiple linear regression\n",
    "    models with only choosing the variables we want to use as the functions\n",
    "    splits the data in train and test, train the model with the training data\n",
    "    and predict the test set using only the columns we choose, and lastly it plots\n",
    "    the errors of the predictions and returns (if we choose that way) the trained\n",
    "    model and a dataframe with train and test errors.    \n",
    "    '''\n",
    "    X_train, y_train, X_test, y_test = ts_train_test_split(X_data, y_data, cols)\n",
    "    \n",
    "    model = LinearRegression(fit_intercept=False).fit(X_train, y_train)\n",
    "    y_pred_train = pd.Series(\n",
    "            model.predict(X_train),\n",
    "            index=X_train.index,\n",
    "            name='Y_pred_train')\n",
    "    y_pred_test = pd.Series(\n",
    "            model.predict(X_test),\n",
    "            index=X_test.index,\n",
    "            name='Y_pred_test')\n",
    "    \n",
    "    error_train, error_test, mae_train, mae_test, mse_train, mse_test, msle_train, msle_test, df_errors = metrics_computation(y_train, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "    plot_metrics_model(cols, y_data, y_pred_train, y_pred_test, error_train, error_test,\n",
    "                       msle_train, msle_test, mse_train, mse_test, mae_train, mae_test, df_errors, title)\n",
    "\n",
    "    if ret==True:\n",
    "        return(model, df_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First iteration\n",
    "\n",
    "- Model 1: `constant`, `trend` \n",
    "- Model 2: `constant`, `trend_squared`\n",
    "\n",
    "We can see that model 1 has a lower MSLE, and the remaining error follows an annual seasonality. Hence, we will keep the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:42.317293Z",
     "iopub.status.busy": "2022-09-07T19:28:42.316497Z",
     "iopub.status.idle": "2022-09-07T19:28:42.941431Z",
     "shell.execute_reply": "2022-09-07T19:28:42.939069Z",
     "shell.execute_reply.started": "2022-09-07T19:28:42.317250Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_linear_reg(X, y, ['const','trend'], 'Model 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:42.943945Z",
     "iopub.status.busy": "2022-09-07T19:28:42.943233Z",
     "iopub.status.idle": "2022-09-07T19:28:43.591136Z",
     "shell.execute_reply": "2022-09-07T19:28:43.589929Z",
     "shell.execute_reply.started": "2022-09-07T19:28:42.943898Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_linear_reg(X, y, ['const','trend_squared'], 'Model 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second iteration: \n",
    "\n",
    "#### Common variables: `const`, `trend`\n",
    "\n",
    "- Model 3: Common varibles + seasonal columns\n",
    "- Model 4: Common variables + fourier series\n",
    "- Model 5: Common variables + fourier series multiplied by linear trend\n",
    "\n",
    "Here the model 5 has the lower MSLE for train and test, but what if we add to it the seasonal columns to give it more flexibility to the fit? We will try this with **Model 6**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:43.593704Z",
     "iopub.status.busy": "2022-09-07T19:28:43.592979Z",
     "iopub.status.idle": "2022-09-07T19:28:44.215324Z",
     "shell.execute_reply": "2022-09-07T19:28:44.214164Z",
     "shell.execute_reply.started": "2022-09-07T19:28:43.593656Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_linear_reg(X, y, ['const','trend'] + seasonal_cols, 'Model 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:44.219663Z",
     "iopub.status.busy": "2022-09-07T19:28:44.219319Z",
     "iopub.status.idle": "2022-09-07T19:28:44.832937Z",
     "shell.execute_reply": "2022-09-07T19:28:44.831669Z",
     "shell.execute_reply.started": "2022-09-07T19:28:44.219630Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_linear_reg(X, y, ['const','trend','sin','cos'], 'Model 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:44.835931Z",
     "iopub.status.busy": "2022-09-07T19:28:44.835092Z",
     "iopub.status.idle": "2022-09-07T19:28:45.600239Z",
     "shell.execute_reply": "2022-09-07T19:28:45.598967Z",
     "shell.execute_reply.started": "2022-09-07T19:28:44.835894Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_linear_reg(X, y, ['const','trend','sin_tr','cos_tr'], 'Model 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third iteration\n",
    "\n",
    "#### Common variables: `const`\n",
    "\n",
    "- Model 6: trend + Fourier series multiplied by trend + seasonal columns\n",
    "- Model 7: historical mean + Fourier series multiplied by trend + seasonal columns\n",
    "- Model 8: historical mean + Fourier series multiplied by historical mean + seasonal columns\n",
    "\n",
    "When we change the trend (Model 6) with the historical mean (Model 7), we see a significant improvement. If we go a bit further and use Fourier series multiplied by historical mean (Model 8) instead that multiplied by trend (Model 7), we see asmall  improvement in the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:45.602863Z",
     "iopub.status.busy": "2022-09-07T19:28:45.601802Z",
     "iopub.status.idle": "2022-09-07T19:28:46.216769Z",
     "shell.execute_reply": "2022-09-07T19:28:46.215694Z",
     "shell.execute_reply.started": "2022-09-07T19:28:45.602824Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_linear_reg(X, y, ['const','trend','sin_tr','cos_tr'] + seasonal_cols, 'Model 6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:46.218712Z",
     "iopub.status.busy": "2022-09-07T19:28:46.218384Z",
     "iopub.status.idle": "2022-09-07T19:28:46.868518Z",
     "shell.execute_reply": "2022-09-07T19:28:46.867477Z",
     "shell.execute_reply.started": "2022-09-07T19:28:46.218682Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_linear_reg(X, y, ['const','Y_mean_12m','sin_tr','cos_tr'] + seasonal_cols, 'Model 7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:46.870327Z",
     "iopub.status.busy": "2022-09-07T19:28:46.870008Z",
     "iopub.status.idle": "2022-09-07T19:28:47.496777Z",
     "shell.execute_reply": "2022-09-07T19:28:47.495664Z",
     "shell.execute_reply.started": "2022-09-07T19:28:46.870298Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_linear_reg(X, y, ['const','Y_mean_12m','sin_m12w','cos_m12w'] + seasonal_cols, 'Model 8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given these results, we will pick Model 8 as the final model and compare it with the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline vs Final model 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the errors table for the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:47.498614Z",
     "iopub.status.busy": "2022-09-07T19:28:47.498199Z",
     "iopub.status.idle": "2022-09-07T19:28:48.132381Z",
     "shell.execute_reply": "2022-09-07T19:28:48.131231Z",
     "shell.execute_reply.started": "2022-09-07T19:28:47.498582Z"
    }
   },
   "outputs": [],
   "source": [
    "best_lr, errors_lr = mdl_linear_reg(X, y, ['const','Y_mean_12m','sin_m12w','cos_m12w'] + seasonal_cols, 'Model 8', ret=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error comparison\n",
    "\n",
    "We consolidate the different table errors in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:48.134923Z",
     "iopub.status.busy": "2022-09-07T19:28:48.134450Z",
     "iopub.status.idle": "2022-09-07T19:28:48.158748Z",
     "shell.execute_reply": "2022-09-07T19:28:48.157932Z",
     "shell.execute_reply.started": "2022-09-07T19:28:48.134857Z"
    }
   },
   "outputs": [],
   "source": [
    "df_errors = pd.concat([errors_lo.assign(model='Last obs'),\n",
    "                       errors_hmean.assign(model='Historical mean'),\n",
    "                       errors_hmedian.assign(model='Historical median'),\n",
    "                       errors_lr.assign(model='linear regression')])\n",
    "df_errors['metric'] = df_errors['metrics'].str.split(' ', expand=True)[0]\n",
    "df_errors['dataset'] = df_errors['metrics'].str.split(' ', expand=True)[1]\n",
    "mask = df_errors.metric == 'MSLE'\n",
    "df_errors[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:48.160755Z",
     "iopub.status.busy": "2022-09-07T19:28:48.159894Z",
     "iopub.status.idle": "2022-09-07T19:28:48.166635Z",
     "shell.execute_reply": "2022-09-07T19:28:48.165381Z",
     "shell.execute_reply.started": "2022-09-07T19:28:48.160722Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Our Model 8 is better than the best baseline train (Last Obs) in:',round((0.01100-0.00316)/.01100,3)*100,'%')\n",
    "print('Our Model 8 is better than the best baseline test (Last Obs) in:',round((0.01215-0.00186)/0.01215,3)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we plot the three baselined model errors and the errors of the final Model 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-07T19:28:48.168126Z",
     "iopub.status.busy": "2022-09-07T19:28:48.167735Z",
     "iopub.status.idle": "2022-09-07T19:28:50.188670Z",
     "shell.execute_reply": "2022-09-07T19:28:50.187611Z",
     "shell.execute_reply.started": "2022-09-07T19:28:48.168080Z"
    }
   },
   "outputs": [],
   "source": [
    "max_value_MSLE_plots = df_errors.loc[df_errors.metric == 'MSLE', 'values'].max()*1.1\n",
    "max_value_MSE_plots = df_errors.loc[df_errors.metric == 'MSE', 'values'].max()*1.1\n",
    "max_value_MAE_plots = df_errors.loc[df_errors.metric == 'MAE', 'values'].max()*1.1\n",
    "\n",
    "g = sns.catplot(kind='bar', data=df_errors, x='dataset', y='values', col='model', row='metric', sharey=False)\n",
    "[g.axes[0][i].set_ylim(0,max_value_MSLE_plots) for i in range(0,4)]\n",
    "[g.axes[1][i].set_ylim(0,max_value_MSE_plots) for i in range(0,4)]\n",
    "[g.axes[2][i].set_ylim(0,max_value_MAE_plots) for i in range(0,4)]\n",
    "for i in g.axes:\n",
    "    for k in i:\n",
    "        title = k.get_title()\n",
    "        k.set_title(title, size=14)\n",
    "        k.set_xlabel(None)\n",
    "        \n",
    "        y_label = k.get_ylabel()\n",
    "        k.set_ylabel(y_label, size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We ended  with a final model that improves the performance of the baseline model by more than 70%. I stopped there because it's a significant increase, we could have tried further, like using a tree-based model \n",
    "\n",
    "In real-life scenarios usually the baseline model is the alternative the business currently executes (e.g. how today they are predicting demand). For example, if they predict montly demand thorugh different analysis and they run those analysis every month, the performance of those analysis is the alternative we should compare the performance of our Model 8. \n",
    "\n",
    "At a personal level, I like to present these results with business metrics. For example, how much increase in income represents that 70% increase in performance over the baseline model? How do we explain this model from a business perspective? We can take the coefficients of the final model and explain the impact of each feature in the dependent variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
